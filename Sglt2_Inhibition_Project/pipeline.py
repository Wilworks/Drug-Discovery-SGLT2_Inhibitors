# -*- coding: utf-8 -*-
"""Copy of Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ebeSq8tDEV3sO0Ho5VyR_DJsdhtJp9nz
"""

import pandas as pd

# Load the data from the Excel file
df = pd.read_excel('/content/Wilfred.xlsx')

# Drop all columns except for 'PUBCHEM_EXT_DATASOURCE_SMILES' and 'PUBCHEM_ACTIVITY_OUTCOME'
df = df[['PUBCHEM_EXT_DATASOURCE_SMILES', 'PUBCHEM_ACTIVITY_OUTCOME']]

# Remove rows where 'PUBCHEM_ACTIVITY_OUTCOME' is 'Unspecified'
df = df[df['PUBCHEM_ACTIVITY_OUTCOME'] != 'Unspecified'].copy()

# Separate the two classes
df_active = df[df['PUBCHEM_ACTIVITY_OUTCOME'] == 'Active'].copy()
df_inactive = df[df['PUBCHEM_ACTIVITY_OUTCOME'] == 'Inactive'].copy()

# Determine the number of samples in the minority class
n_minority = len(df_active)

# Randomly undersample the majority class
df_inactive_undersampled = df_inactive.sample(n=n_minority, random_state=42)

# Concatenate the undersampled majority class and the minority class
df_balanced = pd.concat([df_active, df_inactive_undersampled])

# Shuffle the balanced dataframe
df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

# Display the new class distribution
display(df_balanced['PUBCHEM_ACTIVITY_OUTCOME'].value_counts())
display(df_balanced.head())

from rdkit import Chem
from rdkit.Chem import AllChem
import numpy as np

def smiles_to_fingerprint(smiles_string):
    """Converts a SMILES string to an RDKit molecular fingerprint."""
    try:
        mol = Chem.MolFromSmiles(smiles_string)
        if mol is not None:
            # Generate ECFP4 fingerprints
            fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)
            return np.array(fingerprint)
        else:
            return None
    except:
        return None

# Apply the function to the 'PUBCHEM_EXT_DATASOURCE_SMILES' column
df_balanced['Fingerprint'] = df_balanced['PUBCHEM_EXT_DATASOURCE_SMILES'].apply(smiles_to_fingerprint)

# Remove rows where fingerprint generation failed
df_balanced = df_balanced.dropna(subset=['Fingerprint']).reset_index(drop=True)

# Convert the list of fingerprints into a NumPy array
X_fingerprints = np.array(df_balanced['Fingerprint'].tolist())

# Display the shape of the fingerprint matrix and the first few rows of the balanced dataframe
display(X_fingerprints.shape)
display(df_balanced.head())

# In this case, there are no other features in df_balanced to combine with the fingerprints
# because only 'PUBCHEM_EXT_DATASOURCE_SMILES' and 'PUBCHEM_ACTIVITY_OUTCOME' were kept
# in the previous steps.
# Therefore, the combined feature matrix is just the fingerprint matrix.
X_combined = X_fingerprints

# Display the shape of the resulting X_combined feature matrix to verify the combination.
display(X_combined.shape)

from sklearn.model_selection import train_test_split

# Define the feature matrix X and the target variable y
X = X_combined
y = df_balanced['PUBCHEM_ACTIVITY_OUTCOME']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Print the shapes of the resulting sets
print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

# Instantiate and train the RandomForestClassifier model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# Instantiate and train the GradientBoostingClassifier model
gb_model = GradientBoostingClassifier(random_state=42)
gb_model.fit(X_train, y_train)

print("Random Forest Model Training Complete.")
print("Gradient Boosting Model Training Complete.")

import shap

# Initialize TreeExplainer for RandomForestClassifier
explainer_rf = shap.TreeExplainer(rf_model)

# Calculate SHAP values for the training data using the RF explainer
shap_values_rf = explainer_rf.shap_values(X_train)

# Initialize TreeExplainer for GradientBoostingClassifier
explainer_gb = shap.TreeExplainer(gb_model)

# Calculate SHAP values for the training data using the GB explainer
shap_values_gb = explainer_gb.shap_values(X_train)

# Print the shape of the calculated SHAP values
print("Shape of SHAP values for RandomForestClassifier:", shap_values_rf[0].shape)
print("Shape of SHAP values for GradientBoostingClassifier:", shap_values_gb.shape)

# Display the first few rows of the SHAP values for both models
print("\nFirst few rows of SHAP values for RandomForestClassifier (for class 0 - Inactive):")
display(shap_values_rf[0][:5])

print("\nFirst few rows of SHAP values for GradientBoostingClassifier:")
display(shap_values_gb[:5])

from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder

# Instantiate and train the RandomForestClassifier model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
print("Random Forest Model Training Complete.")

# Instantiate and train the GradientBoostingClassifier model
gb_model = GradientBoostingClassifier(random_state=42)
gb_model.fit(X_train, y_train)
print("Gradient Boosting Model Training Complete.")

# Encode the target variable for models that require numerical labels
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)

# Instantiate and train the XGBClassifier model
xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
xgb_model.fit(X_train, y_train_encoded)
print("XGBoost Model Training Complete.")

# Instantiate and train the CatBoostClassifier model
cat_model = CatBoostClassifier(random_state=42, verbose=0)
cat_model.fit(X_train, y_train_encoded)
print("CatBoost Model Training Complete.")

# Instantiate and train the SVC model
svm_model = SVC(probability=True, random_state=42)
svm_model.fit(X_train, y_train_encoded)
print("SVM Model Training Complete.")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# Encode the test set target variable
label_encoder = LabelEncoder()
y_test_encoded = label_encoder.fit_transform(y_test)

# Evaluate RandomForestClassifier
y_pred_rf = rf_model.predict(X_test)
y_pred_rf_encoded = label_encoder.transform(y_pred_rf) # Encode predictions for metric calculation

accuracy_rf = accuracy_score(y_test_encoded, y_pred_rf_encoded)
precision_rf = precision_score(y_test_encoded, y_pred_rf_encoded)
recall_rf = recall_score(y_test_encoded, y_pred_rf_encoded)
f1_rf = f1_score(y_test_encoded, y_pred_rf_encoded)
y_prob_rf = rf_model.predict_proba(X_test)[:, 1] # Get probabilities for the positive class
roc_auc_rf = roc_auc_score(y_test_encoded, y_prob_rf)
conf_matrix_rf = confusion_matrix(y_test_encoded, y_pred_rf_encoded)

print("Random Forest Classifier Evaluation:")
print(f"Accuracy: {accuracy_rf:.4f}")
print(f"Precision: {precision_rf:.4f}")
print(f"Recall: {recall_rf:.4f}")
print(f"F1-score: {f1_rf:.4f}")
print(f"ROC AUC: {roc_auc_rf:.4f}")
print("Confusion Matrix:")
display(conf_matrix_rf)

# Evaluate GradientBoostingClassifier
y_pred_gb = gb_model.predict(X_test)
y_pred_gb_encoded = label_encoder.transform(y_pred_gb) # Encode predictions for metric calculation

accuracy_gb = accuracy_score(y_test_encoded, y_pred_gb_encoded)
precision_gb = precision_score(y_test_encoded, y_pred_gb_encoded)
recall_gb = recall_score(y_test_encoded, y_pred_gb_encoded)
f1_gb = f1_score(y_test_encoded, y_pred_gb_encoded)
y_prob_gb = gb_model.predict_proba(X_test)[:, 1] # Get probabilities for the positive class
roc_auc_gb = roc_auc_score(y_test_encoded, y_prob_gb)
conf_matrix_gb = confusion_matrix(y_test_encoded, y_pred_gb_encoded)

print("\nGradient Boosting Classifier Evaluation:")
print(f"Accuracy: {accuracy_gb:.4f}")
print(f"Precision: {precision_gb:.4f}")
print(f"Recall: {recall_gb:.4f}")
print(f"F1-score: {f1_gb:.4f}")
print(f"ROC AUC: {roc_auc_gb:.4f}")
print("Confusion Matrix:")
display(conf_matrix_gb)

# Evaluate XGBClassifier
y_pred_xgb = xgb_model.predict(X_test)
# XGBoost predicts encoded labels directly

accuracy_xgb = accuracy_score(y_test_encoded, y_pred_xgb)
precision_xgb = precision_score(y_test_encoded, y_pred_xgb)
recall_xgb = recall_score(y_test_encoded, y_pred_xgb)
f1_xgb = f1_score(y_test_encoded, y_pred_xgb)
y_prob_xgb = xgb_model.predict_proba(X_test)[:, 1] # Get probabilities for the positive class
roc_auc_xgb = roc_auc_score(y_test_encoded, y_prob_xgb)
conf_matrix_xgb = confusion_matrix(y_test_encoded, y_pred_xgb)

print("\nXGBoost Classifier Evaluation:")
print(f"Accuracy: {accuracy_xgb:.4f}")
print(f"Precision: {precision_xgb:.4f}")
print(f"Recall: {recall_xgb:.4f}")
print(f"F1-score: {f1_xgb:.4f}")
print(f"ROC AUC: {roc_auc_xgb:.4f}")
print("Confusion Matrix:")
display(conf_matrix_xgb)


# Evaluate CatBoostClassifier
y_pred_cat = cat_model.predict(X_test)
# CatBoost predicts encoded labels directly

accuracy_cat = accuracy_score(y_test_encoded, y_pred_cat)
precision_cat = precision_score(y_test_encoded, y_pred_cat)
recall_cat = recall_score(y_test_encoded, y_pred_cat)
f1_cat = f1_score(y_test_encoded, y_pred_cat)
y_prob_cat = cat_model.predict_proba(X_test)[:, 1] # Get probabilities for the positive class
roc_auc_cat = roc_auc_score(y_test_encoded, y_prob_cat)
conf_matrix_cat = confusion_matrix(y_test_encoded, y_pred_cat)

print("\nCatBoost Classifier Evaluation:")
print(f"Accuracy: {accuracy_cat:.4f}")
print(f"Precision: {precision_cat:.4f}")
print(f"Recall: {recall_cat:.4f}")
print(f"F1-score: {f1_cat:.4f}")
print(f"ROC AUC: {roc_auc_cat:.4f}")
print("Confusion Matrix:")
display(conf_matrix_cat)

# Evaluate SVC
y_pred_svm = svm_model.predict(X_test)
# SVM predicts encoded labels directly

accuracy_svm = accuracy_score(y_test_encoded, y_pred_svm)
precision_svm = precision_score(y_test_encoded, y_pred_svm)
recall_svm = recall_score(y_test_encoded, y_pred_svm)
f1_svm = f1_score(y_test_encoded, y_pred_svm)
y_prob_svm = svm_model.predict_proba(X_test)[:, 1] # Get probabilities for the positive class
roc_auc_svm = roc_auc_score(y_test_encoded, y_prob_svm)
conf_matrix_svm = confusion_matrix(y_test_encoded, y_pred_svm)

print("\nSVC Evaluation:")
print(f"Accuracy: {accuracy_svm:.4f}")
print(f"Precision: {precision_svm:.4f}")
print(f"Recall: {recall_svm:.4f}")
print(f"F1-score: {f1_svm:.4f}")
print(f"ROC AUC: {roc_auc_svm:.4f}")
print("Confusion Matrix:")
display(conf_matrix_svm)

from sklearn.metrics import RocCurveDisplay, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import seaborn as sns

# Create confusion matrices
fig_cm, axes_cm = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))
axes_cm = axes_cm.flatten()

# RandomForestClassifier Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rf, display_labels=['Inactive', 'Active'], ax=axes_cm[0])
axes_cm[0].set_title('Random Forest Confusion Matrix')

# GradientBoostingClassifier Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_gb, display_labels=['Inactive', 'Active'], ax=axes_cm[1])
axes_cm[1].set_title('Gradient Boosting Confusion Matrix')

# XGBoost Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test_encoded, y_pred_xgb, display_labels=['Inactive', 'Active'], ax=axes_cm[2])
axes_cm[2].set_title('XGBoost Confusion Matrix')

# CatBoost Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test_encoded, y_pred_cat, display_labels=['Inactive', 'Active'], ax=axes_cm[3])
axes_cm[3].set_title('CatBoost Confusion Matrix')

# SVM Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test_encoded, y_pred_svm, display_labels=['Inactive', 'Active'], ax=axes_cm[4])
axes_cm[4].set_title('SVM Confusion Matrix')

# Hide the unused subplot
fig_cm.delaxes(axes_cm[5])

plt.tight_layout()
plt.show()


# Create ROC curves
fig_roc, ax_roc = plt.subplots(figsize=(10, 8))

# RandomForestClassifier ROC Curve
RocCurveDisplay.from_estimator(rf_model, X_test, y_test, ax=ax_roc, name='Random Forest')

# GradientBoostingClassifier ROC Curve
RocCurveDisplay.from_estimator(gb_model, X_test, y_test, ax=ax_roc, name='Gradient Boosting')

# XGBoost ROC Curve
RocCurveDisplay.from_estimator(xgb_model, X_test, y_test_encoded, ax=ax_roc, name='XGBoost')

# CatBoost ROC Curve
RocCurveDisplay.from_estimator(cat_model, X_test, y_test_encoded, ax=ax_roc, name='CatBoost')

# SVM ROC Curve
RocCurveDisplay.from_estimator(svm_model, X_test, y_test_encoded, ax=ax_roc, name='SVM')


ax_roc.set_title('ROC Curves for Classification Models')
plt.show()

import shap

# Initialize TreeExplainer for RandomForestClassifier
explainer_rf = shap.TreeExplainer(rf_model)

# Calculate SHAP values for the training data using the RF explainer
shap_values_rf = explainer_rf.shap_values(X_train)

# Initialize TreeExplainer for GradientBoostingClassifier
explainer_gb = shap.TreeExplainer(gb_model)

# Calculate SHAP values for the training data using the GB explainer
shap_values_gb = explainer_gb.shap_values(X_train)

# Initialize TreeExplainer for CatBoostClassifier
explainer_cat = shap.TreeExplainer(cat_model)

# Calculate SHAP values for the training data using the CatBoost explainer
shap_values_cat = explainer_cat.shap_values(X_train)

# Print the shape of the calculated SHAP values for each model that succeeded
print("Shape of SHAP values for RandomForestClassifier:", shap_values_rf[0].shape)
print("Shape of SHAP values for GradientBoostingClassifier:", shap_values_gb.shape)
print("Shape of SHAP values for CatBoostClassifier:", shap_values_cat.shape)

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Calculate mean absolute SHAP values for RandomForestClassifier (Class 1 - Active)
# shap_values_rf[1] has shape (n_samples, n_features)
mean_abs_shap_rf = np.mean(np.abs(shap_values_rf[1]), axis=0)

# Create a pandas Series for easier sorting and plotting
mean_abs_shap_rf_series = pd.Series(mean_abs_shap_rf, index=[f'feature_{i}' for i in range(len(mean_abs_shap_rf))])

# Sort the features by mean absolute SHAP value and select the top N
top_n = 20  # Display top 20 features
top_features_rf = mean_abs_shap_rf_series.sort_values(ascending=False).head(top_n)

# Plot the top N mean absolute SHAP values for RandomForestClassifier
plt.figure(figsize=(10, 8))
top_features_rf.plot(kind='barh')
plt.title(f'Top {top_n} Mean Absolute SHAP Values for RandomForestClassifier (Class: Active)')
plt.xlabel('Mean Absolute SHAP Value')
plt.ylabel('Features')
plt.gca().invert_yaxis() # Display the highest values at the top
plt.tight_layout()
plt.show()

# Calculate mean absolute SHAP values for GradientBoostingClassifier
# shap_values_gb has shape (n_samples, n_features)
mean_abs_shap_gb = np.mean(np.abs(shap_values_gb), axis=0)

# Create a pandas Series for easier sorting and plotting
mean_abs_shap_gb_series = pd.Series(mean_abs_shap_gb, index=[f'feature_{i}' for i in range(len(mean_abs_shap_gb))])

# Sort the features by mean absolute SHAP value and select the top N
top_features_gb = mean_abs_shap_gb_series.sort_values(ascending=False).head(top_n)

# Plot the top N mean absolute SHAP values for GradientBoostingClassifier
plt.figure(figsize=(10, 8))
top_features_gb.plot(kind='barh')
plt.title(f'Top {top_n} Mean Absolute SHAP Values for GradientBoostingClassifier')
plt.xlabel('Mean Absolute SHAP Value')
plt.ylabel('Features')
plt.gca().invert_yaxis() # Display the highest values at the top
plt.tight_layout()
plt.show()

# Calculate mean absolute SHAP values for CatBoostClassifier
# shap_values_cat has shape (n_samples, n_features)
mean_abs_shap_cat = np.mean(np.abs(shap_values_cat), axis=0)

# Create a pandas Series for easier sorting and plotting
mean_abs_shap_cat_series = pd.Series(mean_abs_shap_cat, index=[f'feature_{i}' for i in range(len(mean_abs_shap_cat))])

# Sort the features by mean absolute SHAP value and select the top N
top_features_cat = mean_abs_shap_cat_series.sort_values(ascending=False).head(top_n)

# Plot the top N mean absolute SHAP values for CatBoostClassifier
plt.figure(figsize=(10, 8))
top_features_cat.plot(kind='barh')
plt.title(f'Top {top_n} Mean Absolute SHAP Values for CatBoostClassifier')
plt.xlabel('Mean Absolute SHAP Value')
plt.ylabel('Features')
plt.gca().invert_yaxis() # Display the highest values at the top
plt.tight_layout()
plt.show()

import numpy as np
from rdkit import Chem
from rdkit.Chem import AllChem

def smiles_to_fingerprint(smiles_string):
    """Converts a SMILES string to an RDKit molecular fingerprint."""
    try:
        mol = Chem.MolFromSmiles(smiles_string)
        if mol is not None:
            # Generate ECFP4 fingerprints with the same parameters used during training
            fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)
            return np.array(fingerprint)
        else:
            return None
    except:
        return None

def predict_activity(smiles, cid, sid, model, label_encoder):
    """
    Predicts the activity of a compound based on its SMILES string.

    Args:
        smiles (str): The SMILES string of the compound.
        cid (int): The PubChem CID of the compound.
        sid (int): The PubChem SID of the compound.
        model: The trained machine learning model.
        label_encoder: The LabelEncoder used to encode the target variable.

    Returns:
        tuple: A tuple containing the CID, SID, and predicted activity outcome (str),
               or an error message if fingerprint generation fails.
    """
    fingerprint = smiles_to_fingerprint(smiles)

    if fingerprint is None:
        return cid, sid, "Error: Could not generate fingerprint from SMILES."

    # Reshape the fingerprint to be a 2D array for prediction
    fingerprint = fingerprint.reshape(1, -1)

    # Predict the encoded label
    predicted_encoded_label = model.predict(fingerprint)[0]

    # Decode the predicted label back to the original string
    predicted_activity = label_encoder.inverse_transform([predicted_encoded_label])[0]

    return cid, sid, predicted_activity

# Assuming xgb_model is the best performing model and label_encoder is available
# Example usage:
# Test with an active compound from the training data (using its SMILES)
active_smiles = df_balanced[df_balanced['PUBCHEM_ACTIVITY_OUTCOME'] == 'Active']['PUBCHEM_EXT_DATASOURCE_SMILES'].iloc[0]
cid_active = 12345 # Example CID
sid_active = 67890 # Example SID
prediction_active = predict_activity(active_smiles, cid_active, sid_active, xgb_model, label_encoder)
print(f"Prediction for Active Compound (CID: {prediction_active[0]}, SID: {prediction_active[1]}): {prediction_active[2]}")

# Test with an inactive compound from the training data (using its SMILES)
inactive_smiles = df_balanced[df_balanced['PUBCHEM_ACTIVITY_OUTCOME'] == 'Inactive']['PUBCHEM_EXT_DATASOURCE_SMILES'].iloc[0]
cid_inactive = 54321 # Example CID
sid_inactive = 9876 # Example SID # Removed leading zero
prediction_inactive = predict_activity(inactive_smiles, cid_inactive, sid_inactive, xgb_model, label_encoder)
print(f"Prediction for Inactive Compound (CID: {prediction_inactive[0]}, SID: {prediction_inactive[1]}): {prediction_inactive[2]}")

# Test with an invalid SMILES string
invalid_smiles = "invalid_smiles_string"
cid_invalid = 99999
sid_invalid = 88888
prediction_invalid = predict_activity(invalid_smiles, cid_invalid, sid_invalid, xgb_model, label_encoder)
print(f"Prediction for Invalid SMILES (CID: {prediction_invalid[0]}, SID: {prediction_invalid[1]}): {prediction_invalid[2]}")

"""# Comprehensive Report on Classification of Compounds for SGLT2 Inhibition

**Prepared for:** Nunana Kingsley (Tutor) & Prof. Samuel Kwofie (Supervisor)

**Prepared by:** Asumboya Wilfred Ayine, Biomedical Engineering Student, Level 300

**Internship:** AI and ML with Institute of Applied Sciences and Technology

**Date:** [Insert Current Date]

## 1. Introduction

This report details the process of classifying chemical compounds as 'Active' or 'Inactive' for inhibiting SGLT2. The analysis involves data loading, preprocessing, feature engineering from SMILES strings, training of multiple machine learning models, evaluation of model performance, and interpretation of results using SHAP values.

## 2. Data

The analysis utilized data from the "/content/Wilfred.xlsx" file. The relevant columns for this study were 'PUBCHEM_SID', 'PUBCHEM_CID', 'PUBCHEM_EXT_DATASOURCE_SMILES', 'PUBCHEM_ACTIVITY_OUTCOME', and 'Standard Type'.

### 2.1 Data Loading and Initial Exploration

The data was loaded into a pandas DataFrame. Initial exploration revealed the columns and their data types.
"""

# Code used for dropping columns and filtering
# df = df[['PUBCHEM_SID', 'PUBCHEM_CID', 'PUBCHEM_EXT_DATASOURCE_SMILES', 'PUBCHEM_ACTIVITY_OUTCOME', 'Standard Type']]
# df = df[df['PUBCHEM_ACTIVITY_OUTCOME'] != 'Unspecified'].copy()

# Code used for checking class distribution and undersampling
# import matplotlib.pyplot as plt
# import seaborn as sns
# sns.countplot(x='PUBCHEM_ACTIVITY_OUTCOME', data=df)
# plt.title('Distribution of PUBCHEM_ACTIVITY_OUTCOME')
# plt.show()
# df_active = df[df['PUBCHEM_ACTIVITY_OUTCOME'] == 'Active'].copy()
# df_inactive = df[df['PUBCHEM_ACTIVITY_OUTCOME'] == 'Inactive'].copy()
# n_minority = len(df_active)
# df_inactive_undersampled = df_inactive.sample(n=n_minority, random_state=42)
# df_balanced = pd.concat([df_active, df_inactive_undersampled])
# df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)
# display(df_balanced['PUBCHEM_ACTIVITY_OUTCOME'].value_counts())

# Code used for converting SMILES to fingerprints
# from rdkit import Chem
# from rdkit.Chem import AllChem
# import numpy as np
# def smiles_to_fingerprint(smiles_string):
#     try:
#         mol = Chem.MolFromSmiles(smiles_string)
#         if mol is not None:
#             fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)
#             return np.array(fingerprint)
#         else:
#             return None
#     except:
#         return None
# df_balanced['Fingerprint'] = df_balanced['PUBCHEM_EXT_DATASOURCE_SMILES'].apply(smiles_to_fingerprint)
# df_balanced = df_balanced.dropna(subset=['Fingerprint']).reset_index(drop=True)
# X_fingerprints = np.array(df_balanced['Fingerprint'].tolist())
# display(X_fingerprints.shape)

# Code used for splitting data and training models
# from sklearn.model_selection import train_test_split
# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
# from xgboost import XGBClassifier
# from catboost import CatBoostClassifier
# from sklearn.svm import SVC
# from sklearn.preprocessing import LabelEncoder
# X = X_fingerprints
# y = df_balanced['PUBCHEM_ACTIVITY_OUTCOME']
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# label_encoder = LabelEncoder()
# y_train_encoded = label_encoder.fit_transform(y_train)
# y_test_encoded = label_encoder.transform(y_test) # Encode test set as well
# rf_model = RandomForestClassifier(random_state=42)
# rf_model.fit(X_train, y_train)
# gb_model = GradientBoostingClassifier(random_state=42)
# gb_model.fit(X_train, y_train)
# xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
# xgb_model.fit(X_train, y_train_encoded)
# cat_model = CatBoostClassifier(random_state=42, verbose=0)
# cat_model.fit(X_train, y_train_encoded)
# svm_model = SVC(probability=True, random_state=42)
# svm_model.fit(X_train, y_train_encoded)

# Code used for model evaluation
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
# y_pred_rf = rf_model.predict(X_test)
# y_pred_gb = gb_model.predict(X_test)
# y_pred_xgb = xgb_model.predict(X_test)
# y_pred_cat = cat_model.predict(X_test)
# y_pred_svm = svm_model.predict(X_test)

# # Encode predictions for models that don't output encoded labels directly
# y_pred_rf_encoded = label_encoder.transform(y_pred_rf)
# y_pred_gb_encoded = label_encoder.transform(y_pred_gb)

# # Calculate metrics (example for RF, similar for others)
# accuracy_rf = accuracy_score(y_test_encoded, y_pred_rf_encoded)
# precision_rf = precision_score(y_test_encoded, y_pred_rf_encoded)
# recall_rf = recall_score(y_test_encoded, y_pred_rf_encoded)
# f1_rf = f1_score(y_test_encoded, y_pred_rf_encoded)
# y_prob_rf = rf_model.predict_proba(X_test)[:, 1]
# roc_auc_rf = roc_auc_score(y_test_encoded, y_prob_rf)
# conf_matrix_rf = confusion_matrix(y_test_encoded, y_pred_rf_encoded)

# print("Random Forest Classifier Evaluation:")
# print(f"Accuracy: {accuracy_rf:.4f}")
# print(f"Precision: {precision_rf:.4f}")
# print(f"Recall: {recall_rf:.4f}")
# print(f"F1-score: {f1_rf:.4f}")
# print(f"ROC AUC: {roc_auc_rf:.4f}")
# display(conf_matrix_rf)

# ... (similar code for GB, XGB, CatBoost, SVM)

# Code used for generating confusion matrices and ROC curves
# from sklearn.metrics import RocCurveDisplay, ConfusionMatrixDisplay
# import matplotlib.pyplot as plt
# import seaborn as sns

# fig_cm, axes_cm = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))
# axes_cm = axes_cm.flatten()

# ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rf, display_labels=['Inactive', 'Active'], ax=axes_cm[0])
# axes_cm[0].set_title('Random Forest Confusion Matrix')

# # ... (similar code for other models)

# fig_roc, ax_roc = plt.subplots(figsize=(10, 8))
# RocCurveDisplay.from_estimator(rf_model, X_test, y_test, ax=ax_roc, name='Random Forest')
# # ... (similar code for other models)
# ax_roc.set_title('ROC Curves for Classification Models')
# plt.show()

# Code used for calculating SHAP values
# import shap
# explainer_rf = shap.TreeExplainer(rf_model)
# shap_values_rf = explainer_rf.shap_values(X_train)
# # ... (similar code for GB, CatBoost, XGB)

# Code used for plotting mean absolute SHAP values
# import matplotlib.pyplot as plt
# import numpy as np
# import pandas as pd

# mean_abs_shap_rf = np.mean(np.abs(shap_values_rf[1]), axis=0) # For RF, class 1
# mean_abs_shap_rf_series = pd.Series(mean_abs_shap_rf, index=[f'feature_{i}' for i in range(len(mean_abs_shap_rf))])
# top_n = 20
# top_features_rf = mean_abs_shap_rf_series.sort_values(ascending=False).head(top_n)
# plt.figure(figsize=(10, 8))
# top_features_rf.plot(kind='barh')
# plt.title(f'Top {top_n} Mean Absolute SHAP Values for RandomForestClassifier (Class: Active)')
# plt.show()

# # ... (similar code for GB, CatBoost, XGB)

# Code used for inference function
# import numpy as np
# from rdkit import Chem
# from rdkit.Chem import AllChem
# def smiles_to_fingerprint(smiles_string):
#     try:
#         mol = Chem.MolFromSmiles(smiles_string)
#         if mol is not None:
#             fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)
#             return np.array(fingerprint)
#         else:
#             return None
#     except:
#         return None
# def predict_activity(smiles, cid, sid, model, label_encoder):
#     fingerprint = smiles_to_fingerprint(smiles)
#     if fingerprint is None:
#         return cid, sid, "Error: Could not generate fingerprint from SMILES."
#     fingerprint = fingerprint.reshape(1, -1)
#     predicted_encoded_label = model.predict(fingerprint)[0]
#     predicted_activity = label_encoder.inverse_transform([predicted_encoded_label])[0]
#     return cid, sid, predicted_activity

"""## Summary:

### Data Analysis Key Findings

*   All five models (RandomForestClassifier, GradientBoostingClassifier, XGBoost, CatBoost, and SVM) were successfully trained after addressing the need for numerical target variables in some models and installing the missing CatBoost library.
*   The evaluation metrics (Accuracy, Precision, Recall, F1-score, and ROC AUC) and confusion matrices were calculated and displayed for all models, showing strong performance in distinguishing between active and inactive compounds. XGBoost and SVM exhibited slightly better performance in terms of Accuracy, Recall, and F1-score.
*   Confusion matrices and ROC curves were successfully generated and visualized for all models, providing a visual comparison of their classification performance.
*   SHAP values were successfully calculated and visualized as bar plots for the RandomForestClassifier, GradientBoostingClassifier, and CatBoostClassifier models, illustrating the top features influencing their predictions.
*   An inference function was successfully developed to take SMILES, CID, and SID as input, generate molecular features from the SMILES, and predict the activity outcome using the trained XGBoost model.

### Insights or Next Steps

*   Based on the evaluation metrics, XGBoost appears to be the best-performing model among those tested for this specific dataset and feature representation.
*   The SHAP value analysis provides valuable insights into which molecular features are most important for the tree-based models' predictions. Further investigation into these key features could lead to a better understanding of the structural characteristics associated with activity.

"""